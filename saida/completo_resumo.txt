**Histogramas e curvas de frequência**: Histogramas e curvas de frequência são representações gráficas de distribuições de frequência. Os histogramas são construídos dividindo o intervalo de dados em intervalos de largura igual, contando a frequência de dados em cada classe e representando a frequência como barras verticais. Já as curvas de frequência são obtidas conectando os pontos médios das barras do histograma. As formas mais comuns de curvas de frequência são normal (gaussiana), uniforme, bimodal e esqueva.

A curva de frequência normal é definida pela equação:

```
f(x) = (1 / (σ√(2π))) * e^(-(x - μ)² / (2σ²))
```

onde:

* x é o valor do dado
* μ é a média
* σ é o desvio padrão
* e é a base da exponencial natural (aproximadamente 2,71828)

Histogramas e curvas de frequência são usados para visualizar distribuições de dados, identificar padrões e tendências, comparar distribuições e fazer inferências sobre populações de dados.

**Diagrama boxplot**: Um diagrama boxplot, também conhecido como diagrama de caixa e bigodes, é uma representação gráfica que exibe a distribuição de um conjunto de dados. Ele fornece uma visão geral resumida da distribuição, incluindo: mediana; média; quartis (Q1, Q3); intervalo interquartil (IQR); bigodes; e outliers. As fórmulas para calcular a mediana, os quartis e o IQR são: Mediana = (N+1)/2; Quartis (Q1, Q3) = (N+1)/4; Intervalo Interquartil (IQR) = Q3 - Q1. As vantagens de usar um diagrama boxplot são: fornece uma visão geral rápida e abrangente da distribuição dos dados; pode identificar outliers e distorções na distribuição; e permite comparações fáceis entre diferentes conjuntos de dados. As limitações de usar um diagrama boxplot são: não mostra a forma detalhada da distribuição; pode ser difícil interpretar quando há muitos outliers; e não é adequado para conjuntos de dados com distribuições bimodais ou multimodais.

**Avaliação de outliers.**: Outliers são dados que se desviam significativamente do resto dos dados. Para identificá-los, métodos gráficos como diagramas de dispersão e box plots podem ser usados. Métodos estatísticos, como o teste de Chauvenet, Grubbs e Dixon, também são úteis. Medidas como o Desvio Absoluto Médio (MAD), Intervalo Interquartil (IQR) e Fórmula de Fences ajudam a quantificar outliers. Antes de removê-los é importante considerar seu valor informativo, pois podem conter informações valiosas. A avaliação de outliers é essencial para garantir a integridade dos dados e a precisão das análises.

**Técnicas de classificação: Naive Bayes**: Naive Bayes é uma técnica de classificação probabilística que se baseia no Teorema de Bayes. É simples, eficiente e pode ser usada para uma ampla variedade de problemas de classificação, assumindo que os recursos da instância são condicionalmente independentes dada a classe. Naive Bayes calcula a probabilidade posterior de cada classe, dada a instância, e atribui a classe com a probabilidade posterior mais alta.

Pode ser afetado por recursos redundantes ou irrelevantes, porém é simples e fácil de implementar, eficiente computacionalmente, pode lidar com dados de alta dimensão, além de ser robusto ao ruído e outliers.

A probabilidade posterior de uma classe _c_ dada uma instância _x_ é calculada como: P(c | x) = P(x | c) * P(c) / P(x).

É amplamente utilizado em vários domínios, incluindo classificação de texto, detecção de spam, análise de sentimento e diagnóstico médico.

**Técnica de classificação Regressão logística**: A regressão logística é uma técnica de classificação estatística usada para prever a probabilidade de um evento binário. Utiliza uma função logística para modelar a probabilidade do evento e estima os coeficientes da função por meio de métodos de otimização. Os coeficientes indicam a influência das variáveis preditoras na probabilidade do evento, podendo ser positivos ou negativos. Com os coeficientes estimados, a função logística pode prever a probabilidade do evento para novas observações e as observações podem ser classificadas em duas classes com base em um limiar de probabilidade. A regressão logística tem vantagens como gerenciar variáveis preditoras contínuas e categóricas, fornecer probabilidades de eventos, ser facilmente interpretável e robusto contra outliers. No entanto, também possui desvantagens, como assumir uma relação linear entre a probabilidade do evento e as variáveis preditoras, ser sensível a desequilíbrios de classes e requerer um tamanho de amostra relativamente grande.

**Técnica de classificação Redes neurais artificiais**: Redes neurais artificiais (RNAs) são modelos inspirados no cérebro humano e capazes de classificar dados em categorias pré-definidas. Existem diferentes abordagens de classificação, como binária, multivariada e hierárquica. Os modelos de RNA usados para classificação incluem perceptron multicamadas (MLP), redes neurais convolucionais (CNNs) e redes neurais recorrentes (RNNs). Fórmulas importantes incluem a função de ativação, que determina a saída de um neurônio, e a função de perda, que mede a diferença entre as previsões e os rótulos. As etapas da classificação com RNAs são: pré-processamento de dados, treinamento da RNA, avaliação e implantação.

**Técnica de classificação Árvores de decisão (algoritmos ID3 e C4.5)**: Árvores de Decisão são uma técnica de classificação supervisionada que divide iterativamente um conjunto de dados em subconjuntos menores, com cada nó representando um atributo ou característica e cada ramificação representando um possível valor desse atributo. O algoritmo ID3 seleciona o atributo que melhor divide os dados usando a Entropia de Informação. O algoritmo C4.5 é uma extensão do ID3 que usa Gain Ratio, considerando a relação entre a Informação de Ganho e a Entropia de Dados Dividida. As vantagens das Árvores de Decisão incluem facilidade de interpretação, robustez a dados ausentes e barulhentos, capacidade de lidar com dados categóricos e numéricos, e pouca necessidade de pré-processamento. Desvantagens incluem suscetibilidade a sobreajuste se não forem podadas corretamente, complexidade com grandes conjuntos de dados e sensibilidade à ordem dos atributos na construção da árvore.

**Técnica de classificação florestas aleatórias (random forest)**: A técnica de classificação florestas aleatórias é um método de conjunto de aprendizado de máquina que cria um conjunto de árvores de decisão para melhorar a precisão da classificação, selecionando aleatoriamente amostras e construindo uma árvore de decisão para cada amostra inicial. A precisão de cada árvore individual é calculada pela divisão dos verdadeiros positivos pela soma dos verdadeiros positivos e falsos positivos. A precisão da floresta aleatória é calculada pela divisão da soma dos verdadeiros positivos e verdadeiros negativos pela soma de todos os quatro tipos de resultados. A técnica de florestas aleatórias apresenta vantagens como alta precisão, robustez a ruído e valores ausentes além da capacidade de lidar com dados de alta dimensão e mensurar a importância das variáveis. No entanto, também possui desvantagens como o alto custo computacional, a suscetibilidade a superajuste com um número muito grande de árvores e a dificuldade de interpretação dos resultados com muitas árvores.

**Técnica de classificação Máquinas de vetores de suporte (SVM – support vector machines)**: As máquinas de vetores de suporte (SVM) são algoritmos de classificação supervisionados usados para resolver problemas de classificação binária e múltipla. Visam maximizar a margem entre os vetores de suporte e o hiperplano de decisão com a função objetivo:

max w'w
sujeito a: y_i (w'x_i + b) >= 1, para todo i

São altamente eficazes em conjuntos de dados de alta dimensão, robustas a ruídos e sobreajuste, e podem lidar com dados não lineares por meio de kernels.

**Técnica de classificação K vizinhos mais próximos (KNN – K-nearest neighbours).**: A KNN (K-nearest neighbours) é um algoritmo de aprendizado supervisionado que classifica novos pontos de dados com base na proximidade com os pontos de dados de treinamento rotulados.  Para usar a KNN, é preciso selecionar um valor K, que representa o número de vizinhos mais próximos a serem considerados. Em seguida, é preciso calcular as distâncias entre o novo ponto de dados e todos os pontos de dados de treinamento. Depois, é preciso identificar os K pontos de dados de treinamento mais próximos do novo ponto de dados. Finalmente, é preciso determinar a classe predominante entre os K vizinhos mais próximos e atribuir o novo ponto de dados à classe mais frequente. A KNN é simples e fácil de implementar, não requer treinamento e pode lidar com dados com muitas dimensões. No entanto, a KNN pode ser sensível à escolha de K, pode ser lenta em conjuntos de dados grandes e não pode identificar relacionamentos complexos entre recursos. A KNN é usada em aplicações como classificação de texto, reconhecimento de padrões e previsão financeira.

**Técnica de classificação**: As técnicas de classificação organizam um conjunto de dados em grupos distintos, sendo usadas em áreas como mineração de dados e reconhecimento de padrões. Os algoritmos de classificação incluem árvore de decisão, floresta aleatória, rede neural, máquina de vetores de suporte (SVM) e k-vizinhos mais próximos (k-NN). Fórmulas comuns incluem entropia (H), ganho de informação (IG) e coeficiente Kappa, usadas para avaliar o desempenho dos algoritmos. As características a considerar incluem tipo de variáveis, número de classes, tamanho do conjunto de dados e desempenho. As aplicações da classificação incluem identificação de padrões, previsão, agrupamento de clientes, detecção de fraudes e classificação de imagens.

**Avaliação de modelos de classificação: treinamento**: Na avaliação de modelos de classificação, o treinamento é fundamental para determinar seu desempenho e capacidade de generalização. Métricas como precisão, recall, especificidade e curva ROC medem o desempenho do treinamento, enquanto a penalidade L1 e a penalidade L2 são métricas de regularização. A capacidade de generalização é avaliada por meio da validação cruzada, que divide os dados de treinamento em dobras e treina e avalia o modelo em cada dobra, e do conjunto de teste de holdout, que separa uma parte dos dados de treinamento para avaliar o modelo após o treinamento nos dados restantes. As fórmulas para precisão, recall e especificidade são: Precisão = TP / (TP + FP), Recall = TP / (TP + FN) e Especificidade = TN / (TN + FP).

**Avaliação de modelos de classificação: teste**: Para avaliar o desempenho de um modelo de classificação em dados não vistos durante o treinamento, utiliza-se o teste. Esse procedimento consiste em dividir o conjunto de dados em conjuntos de treinamento e teste, treinar o modelo no conjunto de treinamento, prever as classes dos dados de teste e calcular métricas de avaliação que incluem precisão, recall e F1-score. Essas métricas são úteis para entender a capacidade do modelo de prever corretamente as classes e discriminar entre elas. No entanto, é importante considerar que o teste depende da representatividade do conjunto de teste e que as métricas de avaliação podem variar dependendo do contexto e do objetivo da classificação.

**Avaliação de modelos de classificação: validação**: A validação é um passo crucial para garantir que os modelos de classificação sejam generalizáveis para novos dados e avaliar seu desempenho em situações do mundo real. Existem vários métodos comuns de validação, como validação cruzada, validação de subconjunto de validação e amostragem de inicialização. As métricas de desempenho comuns incluem precisão, recall e F1-score, que é calculada pela fórmula F1 = 2 * (Precisão * Recall) / (Precisão + Recall). O objetivo da validação é estimar o erro do modelo em novos dados e evitar o overfitting, que é quando o modelo se ajusta muito aos dados de treinamento específicos. A validação é essencial para avaliar o desempenho e a generalização dos modelos de classificação.

**Avaliação de modelos de classificação: validação cruzada**: A validação cruzada é um método estatístico usado para avaliar modelos de classificação. Ela divide os dados disponíveis em subconjuntos (dobras) e treina o modelo iterativamente em diferentes combinações dessas dobras. Existem vários tipos de validação cruzada, como a **validação cruzada k-dobras**, a **validação cruzada de deixe um de fora** e a **validação cruzada estratificada**. As vantagens da validação cruzada incluem a redução do viés de partição de dados, a obtenção de uma estimativa mais confiável do desempenho do modelo e a possibilidade de avaliação de vários modelos usando o mesmo conjunto de dados. As desvantagens da validação cruzada incluem o alto custo computacional, especialmente para conjuntos de dados grandes, e a variação dos resultados dependendo do número de dobras e da ordem em que as dobras são usadas.

**Avaliação de modelos de classificação: métricas de avaliação - matriz de confusão**: A matriz de confusão é uma métrica de avaliação usada para classificação, representando o desempenho de um modelo de classificação por meio da contagem de predições verdadeiras e falsas para diferentes classes. Ela possui quatro componentes: Verdadeiro Positivo (TP), Falso Negativo (FN), Falso Positivo (FP) e Verdadeiro Negativo (TN). As fórmulas para calcular as métricas de avaliação a partir da matriz de confusão são: Precisão = TP / (TP + FP), Revocação = TP / (TP + FN) e Escore F1 = 2 * Precisão * Revocação / (Precisão + Revocação). A matriz de confusão fornece uma visão abrangente do desempenho do modelo e permite calcular métricas adicionais, mas pode ser tendenciosa para conjuntos de dados desbalanceados e não considera a ordem das predições.

**Avaliação de modelos de classificação: acurácia**: A acurácia é uma métrica chave para avaliar modelos de classificação, medindo a proporção de previsões corretas feitas pelo modelo. Ela é calculada dividindo o número de previsões corretas pelo número total de previsões e pode variar de 0 (nenhuma previsão correta) a 1 (todas as previsões corretas). Uma acurácia de 0,5 indica que o modelo está adivinhando aleatoriamente. A acurácia pode ser tendenciosa para conjuntos de dados desequilibrados e não considera a distribuição de classes prevista, o que pode torná-la pouco informativa para modelos que produzem previsões probabilísticas.

**Avaliação de modelos de classificação: precisão**: Precisão, também conhecida como Valor Preditivo Positivo (PPV), é uma métrica de avaliação em classificação de modelos. Calculada pela divisão entre Verdadeiros Positivos e a soma de Verdadeiros Positivos e Falsos Positivos, indica a proporção de previsões positivas corretas para o número total de previsões positivas feitas. Valores altos indicam que o modelo identifica corretamente os exemplos positivos, enquanto valores baixos indicam muitas previsões positivas falsas. No entanto, a precisão pode ser enviesada por distribuições desequilibradas de classes e não considera os falsos negativos. Fórmulas relacionadas incluem Recall (Sensibilidade), Pontuação F1 e Curva ROC.

**Avaliação de modelos de classificação: revocação**: A revocação, também conhecida como sensibilidade, é uma métrica que mede a capacidade de um modelo de classificação em identificar corretamente as instâncias positivas (verdadeiros positivos) do total de instâncias positivas verdadeiras. É calculada como:

```
Revocação = Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Negativos)
```

Um valor de revocação alto indica que o modelo é bom em identificar corretamente as instâncias positivas, enquanto um valor de revocação baixo indica que o modelo está perdendo muitas instâncias positivas verdadeiras. A revocação é mais adequada para problemas de classificação onde falsos negativos são caros ou indesejáveis. Deve ser usado em conjunto com outras métricas, como precisão e F1-score, para uma avaliação abrangente do modelo.

**Avaliação de modelos de classificação: F1-score**: O F1-score é uma métrica amplamente usada para avaliar a precisão e a revocação de um modelo de classificação binária. A fórmula do F1-score é F1 = 2 * (Precisão * Revocação) / (Precisão + Revocação), onde a precisão é a proporção de exemplos classificados como positivos que são realmente positivos e a revocação é a proporção de exemplos positivos que são classificados corretamente pelo modelo. O F1-score varia de 0 a 1, onde 1 indica uma classificação perfeita e 0 indica uma classificação aleatória. O F1-score é útil quando as classes são balanceadas e quando a precisão e a revocação são igualmente importantes. No entanto, o F1-score pode ser enganoso quando as classes são desbalanceadas, o que pode levar a um alto F1-score mesmo que o modelo esteja classificando incorretamente a maioria dos exemplos.

**Avaliação de modelos de classificação: curva ROC.**: A curva ROC (Receiver Operating Characteristic) é uma medida de desempenho para avaliar modelos de classificação. Plote a taxa de verdadeiros positivos (TPR) contra a taxa de falsos positivos (FPR) em todos os limiares de classificação possíveis. Uma curva perfeita é uma linha diagonal de (0,0) a (1,1). Curvas acima da diagonal são melhores que linhas aleatórias. A área sob a curva (AUC) é uma medida resumida do desempenho. A curva ROC é independente do limiar, robusta a desequilíbrios de classe e permite a comparação de modelos. Fórmulas adicionais: Sensibilidade (Recall): TPR; Especificidade: 1 - FPR; Precisão: TP / (TP + FP).

**Técnicas de regressão: Redes neurais para regressão**: As Redes Neurais (RNs) são uma família de algoritmos de aprendizado de máquina inspirados no funcionamento do cérebro humano e são especialmente eficazes em tarefas que envolvem regressão, onde o objetivo é prever valores contínuos. A arquitetura de uma rede neural para regressão consiste em uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída. Cada camada consiste em nós (neurônios) interconectados por pesos, que são ajustados durante o processo de treinamento para minimizar o erro de previsão, medido pela função de perda. O algoritmo de treinamento mais comum é o de retropropagação, que ajusta iterativamente os pesos para minimizar a função de perda. As redes neurais para regressão oferecem vantagens como não linearidade, alta capacidade e generalização, mas também apresentam desvantagens como excesso de ajuste (overfitting), interpretação difícil e tempo de treinamento longo.

**Árvores de decisão para regressão**: Árvores de decisão para regressão são usadas para prever valores numéricos com base em variáveis preditivas. Elas funcionam dividindo recursivamente o conjunto de dados com base nos valores das variáveis preditivas. A métrica de qualidade da divisão, como ganho de informação ou redução de variância, seleciona as variáveis preditivas e o valor de divisão que criam os subconjuntos mais homogêneos. A previsão para novos dados é realizada percorrendo-os pela árvore de decisão. As fórmulas para ganho de informação e redução de variância são dadas por:

```
Ganho de Informação = H(Y) - H(Y|X)
Redução de Variância = Var(Y) - Var(Y|X)
```

onde:

* H(Y) é a entropia dos valores alvo antes da divisão
* H(Y|X) é a entropia condicional dos valores alvo após a divisão pela variável preditiva X
* Var(Y) é a variância dos valores alvo antes da divisão
* Var(Y|X) é a variância condicional dos valores alvo após a divisão pela variável preditiva X

**Máquinas de vetores de suporte para regressão**: As Máquinas de Vetores de Suporte para Regressão (SVR) são uma adaptação das SVM para tarefas de regressão. O objetivo das SVR é encontrar uma função que minimize o erro de previsão do valor-alvo enquanto mantém bons limites de generalização. Para isso, as SVR mapeiam os dados de entrada para um espaço de características de dimensão superior usando uma função de kernel e aplicam uma função de perda de insensibilidade aos erros de previsão. A função objetivo das SVR é minimizada usando programação quadrática, resultando em uma função de regressão linear no espaço de características. As funções de kernel comuns usadas em SVR incluem Kernel Linear, Kernel Polinomial e Kernel Gaussiano Radial. As SVR são robustas a outliers, apresentam boa generalização e podem lidar com dados de alta dimensão. No entanto, elas podem ter treinamento lento para grandes conjuntos de dados, a seleção dos parâmetros pode ser desafiadora e não fornecem estimativas probabilísticas.

**Ajuste de modelos dentro e fora de amostra e overfitting.**: O ajuste de modelos envolve treinar um modelo usando um conjunto de dados e, em seguida, avaliar seu desempenho em novos dados. O ajuste dentro da amostra é feito usando o conjunto de dados completo, enquanto o ajuste fora da amostra é feito usando apenas uma parte dos dados (conjunto de treinamento), enquanto a outra parte (conjunto de teste) é usada para avaliar o desempenho do modelo. Overfitting ocorre quando um modelo se encaixa muito bem aos dados de treinamento, mas não generaliza bem para novos dados. Pode ser causado por dados de treinamento muito pequenos ou ruidosos ou por um modelo muito complexo. As técnicas para evitar overfitting incluem validação cruzada, regularização e seleção de modelo.

**Técnicas de agrupamento:**: As técnicas de agrupamento são algoritmos estatísticos que dividem um conjunto de dados em grupos (clusters) com base em suas semelhanças. Elas visam identificar padrões ou estruturas subjacentes nos dados, facilitando a compreensão e análise. Existem dois tipos principais de técnicas de agrupamento: hierárquicas e não hierárquicas. As técnicas hierárquicas começam com cada ponto como um cluster separado e gradualmente os mesclam com base em sua proximidade ou dividem-os gradualmente com base em sua dissimilitude. As técnicas não hierárquicas atribuem pontos a k clusters iniciais e iterativamente atualizam as médias do cluster e as atribuições de pontos para minimizar a soma dos quadrados das distâncias entre os pontos e seus respectivos centróides ou agrupam pontos com a distância de ligação mais curta ou longa entre eles. A escolha da distância ou medida de semelhança afeta os resultados do agrupamento. Exemplos comuns incluem a distância euclidiana, a distância de Manhattan e o coeficiente de correlação.

**Agrupamento por partição**: O agrupamento por partição, também conhecido como k-means, é um algoritmo de agrupamento não supervisionado que categoriza dados multidimensionais em grupos diferentes. O algoritmo inicializa aleatoriamente um conjunto de centróides, pontos representativos para cada grupo. Em seguida, cada ponto de dado é atribuído ao grupo cuja centróide é mais próxima. As centróides são então atualizadas como as médias das suas atribuições e o processo se repete até que as centróides não mudem significativamente ou um número especificado de iterações seja alcançado. O agrupamento por partição é simples, fácil de implementar, escala bem para grandes conjuntos de dados e pode lidar com dados multidimensionais. Entretanto, requer a especificação antecipada do número de grupos, é sensível à seleção inicial de centróides e pode convergir para soluções locais em vez de soluções ideais. Fórmulas utilizadas: distância euclidiana `dist(p1, p2) = sqrt((p1[0] - p2[0])^2 + (p1[1] - p2[1])^2 + ... + (p1[n] - p2[n])^2)` e nova centróide, `centroid = (1/n) * (p1 + p2 + ... + pn)`.

**Agrupamento por densidade**: O agrupamento por densidade é um método de agrupamento sem supervisão que identifica clusters com base na densidade de pontos de dados no espaço de dados. O algoritmo DBSCAN, que é um exemplo de algoritmo de agrupamento por densidade, considera dois parâmetros: raio de vizinhança (ε) e o número mínimo de pontos em uma vizinhança para considerá-la densa (minPts). O método atribui pontos a clusters com base em sua proximidade e distância de outros pontos. Ele pode identificar clusters de forma arbitrária, pode lidar com dados de ruído e não requer o número de clusters a serem especificados com antecedência. O agrupamento por densidade pode descobrir clusters hierárquicos e tem aplicações em detecção de anomalias, segmentação de imagens, análise de redes sociais e mineração de dados espacial.

**Agrupamento hierárquico.**: O agrupamento hierárquico é um método de agrupamento que cria uma representação hierárquica dos dados, construindo uma árvore (dendrograma) a partir de observações individuais. Cada nó da árvore representa um cluster; e o nível do nó na árvore indica a distância entre os clusters. O processo começa inicializando cada observação como um cluster individual, calculando as distâncias entre os clusters e ligando os dois clusters mais próximos para formar um novo cluster. Esse processo é repetido até que todos os itens sejam agrupados em um único cluster. Existem vários tipos de ligação, como a ligação única, completa, média, por média ponderada e de Ward. O agrupamento hierárquico é fácil de implementar e interpretar, e é usado em aplicações como bioinformática, marketing, análise exploratória de dados e classificação não supervisionada.

**Técnica de redução de dimensionalidade: Seleção de características (feature selection)**: A seleção de características é uma técnica importante de redução de dimensionalidade que envolve escolher um reduzido conjunto de recursos informativos para treinamento de modelo. Ela pode reduzir a complexidade do modelo, melhorar seu rendimento e remover recursos irrelevantes ou ruidosos. Existem vários métodos de seleção de recursos, como filtragem, embutimento e envoltório. A filtragem avalia recursos individualmente com base em sua relevância ou correlação com a variável de resposta. O embutimento, por sua vez, seleciona características dentro do algoritmo de aprendizado. Já o envoltório utiliza um algoritmo de otimização para selecionar o conjunto ideal de características. A seleção de características pode ser benéfica em reduzir sobreajuste, aumentar a interpretabilidade do modelo e melhorar a eficácia de treinamento, mas pode ser intensiva em termos de tempo para grandes conjuntos de recursos, além de representar o risco de remover recursos importantes se selecionados incorretamente.

**Técnicas de redução de dimensionalidade: análise de componentes principais (PCA – principal component analysis).**: A análise de componentes principais (PCA) é uma técnica de redução de dimensionalidade que preserva a variância essencial de um conjunto de dados. Projeta os dados em um novo espaço ortogonal onde as primeiras direções (componentes principais) capturam a maior quantidade de variância. As componentes principais são combinações lineares das variáveis originais.

**Etapas:**

1. Calcule a matriz de covariância.
2. Calcule os autovalores e autovetores da matriz de covariância.
3. Ordene os autovalores por magnitude decrescente e selecione os k maiores autovetores correspondentes.
4. Projete os dados originais nos k componentes principais usando os autovetores selecionados.

**Vantagens:**

* Preserva a variância máxima
* Computacionalmente eficiente
* Não requer informações de classe
* Pode ser facilmente interpretada

**Desvantagens:**

* Pode não capturar algumas informações importantes
* Não garante que a estrutura dos dados seja linear

**Processamento de linguagem natural: Normalização textual**: A normalização textual é uma etapa essencial no processamento de linguagem natural (PNL) que envolve técnicas para tornar o texto mais uniforme e consistente. Essas técnicas incluem a remoção de pontuação, tokenização, stemming e lematização. A normalização textual melhora a precisão dos modelos de PNL, facilita o processamento, aumenta a generalização e reduz a complexidade computacional.

**Técnicas de Normalização Textual**

* Remoção de Pontuação: Remove pontuação desnecessária, como vírgulas, pontos e parênteses.
* Tokenização: Divide o texto em unidades discretas chamadas tokens, geralmente palavras ou caracteres.
* Stemming: Reduz as palavras a sua forma raiz para melhorar a generalização e reduzir o ruído.
* Lematização: Reduz as palavras a um lema canônico (uma forma representativa e dicionarizada) para eliminar variações morfológicas.

**Fórmulas**

* Fórmula de Stemming de Porter: Um algoritmo comum de stemming que remove sufixos comuns de palavras em inglês.
* Algoritmo de Lematização de WordNet: Um algoritmo que usa o WordNet (um dicionário semântico) para identificar o lema de uma palavra.

**Benefícios da Normalização Textual**

* Melhora a precisão: Reduz ruído e variações no texto, resultando em modelos de PNL mais precisos.
* Facilita o processamento: Cria um texto mais uniforme, permitindo que os algoritmos de PNL processem o texto de forma mais eficiente.
* Aumenta a generalização: Permite que os modelos de PNL generalizem para novos textos, mesmo que contenham variações linguísticas.
* Reduz a complexidade: Torna o texto mais gerenciável e reduz a complexidade computacional durante o processamento.

**Processamento de linguagem natural: stop words**: As stop words são palavras comuns que ocorrem com alta frequência em um idioma, mas que geralmente não transmitem muito significado ou valor informativo específico. Elas são removidas do texto durante o processamento de linguagem natural (PNL) para melhorar a eficiência dos algoritmos de PNL.

**Razões para Remover Stop Words:**

* **Redução da dimensionalidade:** As stop words representam uma grande proporção do texto, mas não contribuem significativamente para o significado. Sua remoção reduz a dimensionalidade do texto.
* **Melhoria da precisão:** As stop words podem causar ruído nos algoritmos de PNL, tornando mais difícil extrair recursos informativos do texto.
* **Aceleração do processamento:** Remover stop words reduz o tamanho do texto, acelerando os algoritmos de PNL.

**Fórmulas:**

A frequência de uma palavra de parada (f) pode ser calculada usando a fórmula:

```
f = N(w) / N
```

onde:

* N(w) é o número de ocorrências da palavra w no texto
* N é o número total de palavras no texto

**Exemplos Comuns de Stop Words:**

Em inglês, alguns exemplos comuns de stop words incluem:

* a, an, the
* of, to, in
* is, are, was

**Métodos de Remoção de Stop Words:**

* **Listas de stop words:** Listas pré-compiladas de stop words são usadas para identificar e remover essas palavras do texto.
* **Remoção estatística:** Palavras com frequência extremamente alta (por exemplo, f > 0,9) são consideradas stop words e removidas.
* **Aprendizagem de máquina:** Algoritmos de aprendizado de máquina podem ser treinados para identificar stop words com base em dados de texto.

**Processamento de linguagem natural: estemização**: **Estemização**

A estemização é um processo de redução de palavras à sua forma raiz ou radical, conhecida como esteme. Ela simplifica palavras para análise mais fácil, normalizando-as para melhorar a recuperação de documentos e agrupamento.

**Objetivos**

* Normalizar palavras para melhorar a recuperação e agrupamento de documentos.
* Reduzir a complexidade do vocabulário para processamento mais eficiente.
* Identificar conceitos ou ideias subjacentes, independentemente da variação de palavras.

**Algoritmos**

* **Algoritmo de Porter:** Remove sufixos comuns e aplica exceções para obter a forma base da palavra.
* **Algoritmo Lovins:** Semelhante ao algoritmo de Porter, mas mais abrangente e adequado para textos técnicos.
* **Algoritmo de Lancaster:** Mais complexo, manipula palavras usando uma estrutura de árvore.

**Fórmula**

Fórmulas para estemização são complexas, envolvendo regras linguísticas e exceções. Uma fórmula geral para remover um sufixo:

```
palavra[0:n]
```

* **palavra[0:n]** é a subcadeia da palavra desde o início até a posição do sufixo a ser removido.

**Processamento de linguagem natural: lematização**: A lematização é uma técnica de Processamento de Linguagem Natural (PLN) que converte palavras flexionadas ou derivados de palavras na sua forma canónica ou lema. Visa reduzir a redundância lexical, melhorar a correspondência de padrões e aumentar a precisão de tarefas de PLN. Existem fórmulas comuns de lematização, como o Algoritmo de Porter, o Algoritmo de Lancaster e o WordNet Lemmatizer. A lematização envolve tokenização, remoção de sufixos e prefixos e identificação do lema. Ela é útil em várias tarefas de PLN, incluindo indexação e recuperação de informação, análise de sentimentos, resumo de texto e reconhecimento de entidades nomeadas, pois melhora a precisão ao normalizar o texto e reduzir a redundância.

**Processamento de linguagem natural: análise de frequência de termos**: **Análise de Frequência de Termos em PNL**

A análise de frequência de termos é uma técnica usada em PNL para analisar a ocorrência de termos em um texto. Ela quantifica a importância relativa dos termos, fornecendo insights sobre o conteúdo e a estrutura do texto.

**Procedimento:**

1. **Tokenização:** Dividir o texto em unidades individuais (tokens).
2. **Remoção de stop words:** Remover palavras comuns irrelevantes.
3. **Stemming ou Lematização:** Reduzir as palavras a suas formas básicas.
4. **Contagem de frequência:** Contar o número de ocorrências de cada termo exclusivo.

**Fórmulas:**

* **Frequência Relativa (RF):**

```
RF(t) = (N(t) / N) * 100
```

* **Frequência Inversa de Documentos (IDF):**

```
IDF(t) = log(N / df(t))
```

* **TF-IDF:**

```
TF-IDF(t) = RF(t) * IDF(t)
```

**Aplicações:**

* Classificação de texto
* Extração de palavras-chave
* Resumo de texto
* Detecção de plágio

**Benefícios:**

* Fornece uma representação numérica do conteúdo do texto.
* Identifica termos importantes e tópicos emergentes.
* Ajuda a entender a estrutura e a organização do texto.

**Rotulação de partes do discurso: part-of-speech tagging**: A rotulação de partes do discurso (POS) é o processo de atribuir uma categoria gramatical a cada palavra em uma frase. Ela fornece informações linguísticas estruturais e melhora a precisão de tarefas posteriores de PNL, como análise sintática e semântica. As técnicas de POS incluem métodos baseados em regras, estatísticos e híbridos. Algumas etiquetas comuns são substantivo, verbo, adjetivo, advérbio, preposição, determinante, pronome, conjunção e interjeição. O desempenho da rotulação de POS é medido por precisão e revocação. As aplicações incluem análise sintática, reconhecimento de entidade nomeada, resumo de texto e tradução automática.

Fórmulas:

* Precisão: Precisão = Etiquetas corretas / Etiquetas atribuídas
* Revocação: Revocação = Etiquetas corretas / Etiquetas esperadas

**Modelos de representação de texto: N-gramas**: Os N-gramas são um modelo de representação de texto que fragmenta o texto em subsequências de comprimento fixo, chamadas de n-gramas. Um n-grama é uma sequência de n tokens consecutivos. Existem diferentes tipos de n-gramas, como unigramas (n=1), bigramas (n=2) e trigramas (n=3). Eles capturam sequências ordenadas de tokens e podem ser usados para modelagem de linguagem, classificação de texto e tradução automática. Os n-gramas de ordem superior captam relacionamentos mais complexos entre tokens. No entanto, são sensíveis à ordem dos tokens, o que pode ser uma vantagem ou desvantagem dependendo da tarefa. Os n-gramas são simples e fáceis de implementar, e podem identificar padrões locais no texto. Porém, podem ser esparsos, especialmente para n-gramas de ordem superior, e podem ser afetados por dados esparsos, o que pode levar a problemas de superajuste. Além disso, eles não capturam a estrutura hierárquica do texto. Apesar de suas limitações, os n-gramas permanecem uma ferramenta valiosa para várias tarefas de processamento de linguagem natural.

**modelos vetoriais de palavras: CBOW**: O CBOW é um modelo vetorial de palavras que prevê a palavra atual com base nas palavras de contexto. Ele usa uma arquitetura de entrada-saída com camadas de projeção, soma e saída. Os embeddings das palavras de contexto são somados e passados para uma camada densa para prever a palavra-alvo. O CBOW captura o significado semântico das palavras e é eficiente para treinar e usar. No entanto, ignora a ordem das palavras de contexto e pode ter dificuldades para representar palavras raras.

**modelos vetoriais de palavra: Skip-Gram**: O Skip-Gram é um modelo vetorial de palavra que aprende relacionamentos semânticos e contextuais a partir de uma coleção de textos. Ele prevê a ocorrência de uma palavra com base em suas palavras vizinhas usando a fórmula:

```
P(w_t | w_1, ..., w_k) = softmax(U'v_t)
```

onde U é uma matriz de projeção e v_t é o vetor embutido da palavra-alvo w_t. O modelo é treinado usando retropropagação para minimizar a perda de entropia cruzada.

Ele é gerenciável, mesmo para grandes conjuntos de dados, rápido de treinar e fácil de implementar, tornando-o popular em tarefas de processamento de linguagem natural, aprendizagem de máquina supervisionada, modelagem de tópicos, resumo de texto e geração de linguagem natural.

**modelos vetoriais de palavra: GloVe**: O GloVe é um modelo de representação de palavras que captura as relações semânticas e sintáticas das palavras. Ele é treinado em um grande corpus de texto, como o Wikipedia, e usa uma função de perda de mínimos quadrados ponderados (L2) para minimizar a soma das diferenças quadradas entre as probabilidades previstas e reais. O GloVe considera tanto coocorrências globais (em todo o corpus) quanto locais (em janelas de contexto), permitindo que ele capture relações semânticas e sintáticas. Os vetores GloVe são tipicamente codificados em uma dimensão baixa (e.g., 100, 300), facilitando seu uso em aplicações de aprendizado de máquina. Eles são amplamente utilizados em uma variedade de aplicações de processamento de linguagem natural, incluindo classificação de texto, geração de linguagem natural e resumo de texto.

**modelos vetoriais de documentos: booleano**: Os modelos vetoriais booleanos de documentos são técnicas de representação e comparação de documentos que utilizam vetores binários. Cada componente do vetor indica a presença (1) ou ausência (0) do termo correspondente no documento. A similaridade entre documentos é determinada pela medida de similaridade de Jaccard, que é a proporção de termos em comum entre eles. Esses modelos são simples de calcular e fáceis de interpretar, mas não consideram a frequência dos termos ou a ordem das palavras nos documentos. São usados em aplicações como pesquisa de documentos binários e sistemas de recuperação de informações onde a precisão é crucial.

**modelos vetoriais de documentos: TF**: Os modelos vetoriais de documentos representam documentos como vetores em um espaço n-dimensional, onde n é o número de termos únicos no corpus. A frequência de termos (TF) é uma medida do número de ocorrências de um determinado termo em um documento.

A fórmula para calcular o TF é:

```
TF(t, d) = número de ocorrências do termo t no documento d / número total de termos no documento d
```

O TF mede a importância relativa de um termo dentro de um documento específico, mas não considera a importância global do termo no corpus. Valores mais altos de TF indicam maior relevância do termo para o documento. O TF pode ser modificado para dar maior peso a termos que aparecem no início ou final do documento.

Embora simples e intuitivo, o TF tem algumas limitações. Ele não captura a ordem dos termos no documento, não considera o contexto dos termos e pode ser distorcido por documentos muito longos ou curtos.

O TF é usado em uma variedade de aplicações, incluindo indexação de documentos, recuperação de informações e agrupamento de documentos.

**modelos vetoriais de documentos: TF-IDF**: **Modelos Vetoriais de Documentos: TF-IDF**

Os Modelos Vetoriais de Documentos (TF-IDF) são representações matemáticas de documentos que capturam sua similaridade usando vetores numéricos. Eles são amplamente utilizados em recuperação de informações e análise de texto.

**TF - Frequência de Termo**

A frequência de termo (TF) mede o número de ocorrências de um termo em um documento. É calculado como:

```
TF(t, d) = número de ocorrências do termo t no documento d
```

**IDF - Frequência Inversa do Documento**

A frequência inversa do documento (IDF) mede o quão comum um termo é na coleção de documentos. Ele é calculado como:

```
IDF(t, D) = log((|D| + 1) / (df(t) + 1))
```

onde:

* |D| é o número de documentos na coleção
* df(t) é o número de documentos que contêm o termo t

**Peso TF-IDF**

O peso TF-IDF é o produto da TF e IDF. Ele mede a importância de um termo em um documento específico em relação à coleção inteira. É calculado como:

```
TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)
```

**Vantagens:**

* Captura a importância relativa dos termos em um documento
* Reduz a distorção causada por termos comuns
* Permite a comparação e busca de documentos

**Limitações:**

* Não considera a ordem ou proximidade dos termos
* Pode ser sensível a termos raros ou muito comuns

**modelos vetoriais de documentos: média de vetores de palavras**: **Modelos Vetoriais de Documentos: Média de Vetores de Palavras**

Os modelos vetoriais são uma representação numérica de documentos que capturam semelhanças semânticas. O modelo de vetores de palavras médias é um tipo de modelo vetorial que representa um documento como um vetor, onde cada elemento é a média dos vetores de palavras que ocorrem no documento.

**Vantagens:**

* Captura relações semânticas entre palavras
* Simples de calcular e implementar
* Efetivo para tarefas de classificação de documentos

**Desvantagens:**

* Ignora a ordem das palavras e a sintaxe
* Pode ser sensível a palavras raras e comuns

**Fórmula:**

```
m_d = (1/|d|) * Σ(v_i, i ∈ d)
```

onde:

* `m_d` é o vetor de palavras médias para o documento `d`
* `v_i` é o vetor de palavras para a palavra `i` no vocabulário `V`
* `|d|` é o número de palavras em `d`

**modelos vetoriais de documentos: Paragraph Vector**: **Modelos Vetoriais de Documentos: Paragraph Vector**

**Princípio Fundamental:**

- Paragraph Vector assume que os parágrafos em um documento estão próximos no espaço semântico.
- Usa uma rede neural convolucional para aprender representações vetoriais fixas para parágrafos.

**Arquitetura da Rede:**

- Camada de Embutimento: Embuta cada palavra em um vetor.
- Camada Convolucional: Aplica filtros convolucionais sobre os vetores de embutimento para extrair recursos.
- Camada de Max Pooling: Seleciona o recurso máximo de cada janela convolucional.
- Camada Linear: Projeta os vetores max pooling em um espaço vetorial de dimensão fixa.

**Fórmulas:**

```
p = W * max(conv(x)) + b
```

onde:

* `p` é o vetor de representação do parágrafo
* `W` é a matriz de projeção
* `b` é o vetor de deslocamento
* `conv(x)` é a saída da camada convolucional
* `max()` é a operação de max pooling

**Aplicações:**

- Recuperação de informações
- Classificação de documentos
- Resumo automático
- Modelagem de tópicos

**Vantagens:**

- Captura relações semânticas entre parágrafos.
- Pode ser usado para representar documentos longos com estruturas complexas.
- Produz representações fixas e de comprimento fixo.

**Métricas de similaridade textual - similaridade do cosseno**: A similaridade do cosseno é uma medida de similaridade entre dois vetores de representação textual. Calcula-se o cosseno do ângulo entre os vetores. Quanto maior o valor da similaridade, mais semelhantes são os textos. A similaridade do cosseno é intuitiva e fácil de interpretar, robusta a diferenças na frequência das palavras e eficiente para calcular. No entanto, pode ser sensível ao comprimento do texto, pode não considerar a ordem das palavras e pode produzir resultados inesperados.

**Métricas de similaridade textual distância euclidiana**: As métricas de similaridade textual são usadas para medir a semelhança entre dois textos. Uma das métricas mais utilizadas é a distância euclidiana, que é calculada como a raiz quadrada da soma das diferenças quadráticas entre os vetores de frequência das palavras nos dois textos. Quanto menor a distância, mais semelhantes são os textos.

A distância euclidiana é simples de calcular e pode detectar diferenças significativas na frequência das palavras. No entanto, ela ignora a ordem das palavras e a estrutura sintática, o que pode levar a resultados inexatos em alguns casos.

A distância euclidiana é usada em várias aplicações, como filtragem de spam de e-mail, agrupamento de documentos e detecção de plágio.

**Fórmula da distância euclidiana:**

```
d(x, y) = √(Σ(xᵢ - yᵢ)²)
```

onde:

* **x** e **y** são vetores com o mesmo número de dimensões
* **xᵢ** e **yᵢ** são os valores do **i**-ésimo elemento em **x** e **y**, respectivamente

**Métricas de similaridade textual similaridade de Jaccard**: A similaridade de Jaccard é uma métrica de similaridade textual que mede a proporção de elementos comuns entre dois conjuntos. É usada em várias aplicações de processamento de linguagem natural, incluindo busca de informações, agrupamento de texto e detecção de plágio.

**Fórmula:**

```
Similaridade de Jaccard = |A ⋂ B| / |A ∪ B|
```

onde:

* |A ⋂ B| é o número de elementos comuns aos dois conjuntos
* |A ∪ B| é o número total de elementos distintos nos dois conjuntos

**Vantagens:**

* Simples de calcular
* Insensível à ordem dos elementos

**Desvantagens:**

* Pode ser sensível ao tamanho do conjunto
* Não considera a similaridade semântica entre palavras ou termos

**Métricas de similaridade textual distância de Manhattan**: A distância de Manhattan, também conhecida como distância de bloqueio urbano ou distância da cidade, é uma métrica de similaridade textual que mede a diferença entre duas sequências de caracteres, contando o número de caracteres que precisam ser adicionados, removidos ou substituídos para que as duas sequências sejam iguais. É calculada pela fórmula:

```
d(x, y) = Σ|x_i - y_i|
```

Onde x e y são as duas sequências de caracteres e x_i e y_i são os caracteres correspondentes nas posições i.

A distância de Manhattan é uma métrica simples e eficiente de calcular, tornando-a uma escolha popular para aplicações de processamento de texto, como comparação de documentos, detecção de plágio e classificação de texto.

**Métricas de similaridade textual coeficiente de Dice.**: O coeficiente de Dice é uma métrica de similaridade textual utilizada para medir o grau de sobreposição entre dois conjuntos de tokens (palavras ou elementos distintos).
É definido como:

* Coeficiente de Dice = 2 * Interseção(A, B) / (|A| + |B|)

onde:

* A e B são os conjuntos de tokens
* Interseção(A, B) é o número de tokens comuns a ambos os conjuntos
* |A| e |B| são os tamanhos dos conjuntos A e B, respectivamente

O coeficiente de Dice varia de 0 a 1. Um valor de 1 indica que os conjuntos são idênticos, enquanto um valor de 0 indica que não há sobreposição.

O coeficiente de Dice é usado em várias aplicações, incluindo:

* Recuperação de informações: Medir a semelhança entre documentos ou consultas
* Processamento de linguagem natural: Agrupamento de texto, extração de entidades e resumo
* Ciência da computação: Detecção de plágio, comparação de algoritmos e análise de código-fonte

**Vantagens**:

* Simples de calcular
* Robust a variações na ordem dos tokens
* Maneira útil de quantificar a sobreposição de conjuntos de tokens

**Desvantagens**:

* Não considera a distância entre os tokens
* Pode ser afetado pelo comprimento dos conjuntos
* Não leva em consideração a frequência dos tokens

**Redes neurais convolucionais**: As Redes Neurais Convolucionais (CNNs) são um tipo de rede neural artificial especializadas em processamento de dados grid-like, como imagens e matrizes. Elas empregam operações de convolução para capturar padrões locais. As CNNs apresentam arquiteturas com camadas alternadas de convolução, pooling e totalmente conectadas. Elas são amplamente utilizadas em aplicações como reconhecimento de imagem e objeto, processamento de linguagem natural e análise biomédica. Benefícios incluem: extração automática de características, tolerância a variações e alta precisão. Limitações incluem: requisitos de dados grandes, alta complexidade computacional e interpretabilidade limitada.

**Redes neurais recorrentes.**: Redes Neurais Recorrentes (RNNs) são projetadas para processar dados sequenciais, como texto, fala e dados de séries temporais. Elas possuem conexões recorrentes que permitem que carreguem informações de entradas anteriores para entradas subsequentes. 

Uma célula de memória em uma RNN mantém um estado interno **h**, atualizado pela função de transição: **h(t) = f(Wx + Uh(t-1))**
 **W** e **U** são matrizes de pesos, **f** é a função não linear e **h** é o estado anterior. A saída da RNN é computada a partir do estado atual da célula de memória: **y(t) = g(Vh(t))** onde **V** é matriz de pesos, **g** é a função de ativação. 

Existem tipos de RNNs para processamento de dados sequenciais com comprimentos variáveis, processamento de dados sequenciais em ambas as direções e células de memória aprimoradas que podem lidar com dependências de longo prazo.

As RNNs são amplamente utilizadas em processamento de linguagem natural, reconhecimento de fala, geração de texto e previsão de séries temporais.

**Scikit-learn**: Scikit-learn é uma biblioteca de aprendizado de máquina de código aberto para Python, com algoritmos e ferramentas para tarefas de aprendizado de máquina. Possui algoritmos de aprendizado supervisionado, como classificação (árvores de decisão, regressão logística) e regressão (regressão linear, árvores de regressão), e não supervisionado, como clustering (k-means, hierárquico) e redução de dimensionalidade (PCA). Oferece pré-processamento e transformação de dados, avaliação de modelos, integração com outras bibliotecas e é eficiente, fácil de usar e extensível. Pode ser usado para diversas tarefas, incluindo classificação, regressão, clustering e redução de dimensionalidade. Algumas fórmulas usadas são a de classificação logística, onde p é a probabilidade de pertencer à classe 1, w é o vetor de pesos e x é o vetor de características, e a de árvores de regressão, onde y_pred é a previsão, alpha_i são os pesos dos nós folhas e h(x, s_i) é a função de partição do nó folha com divisão s_i.

**TensorFlow**: TensorFlow é uma plataforma livre e de código aberto de aprendizado de máquina do Google. Ele pode ser usado para construir e treinar modelos de aprendizado de máquina para uma ampla variabilidade de tarefas, incluindo aprendizado supervisionado, não supervisionado e por reforço. 

O TensorFlow é construído em torno do conceito de tensores, que são arrays multidimensionais. Os tensores fluem através de um gráfico computacional, que define as operações a serem executadas nos dados.

O TensorFlow possui uma arquitetura flexível, escalável, eficiente e tem uma comunidade ativa. 

Códigos exemplares são encontrados na documentação e a plataforma pode ser utilizada em variadas aplicações, como reconhecimento de imagem, processamento de linguagem natural, previsão de séries temporais, aprendizado por reforço e bioinformática.

**PyTorch**: PyTorch é uma estrutura de aprendizado profundo open-source, baseada em Python, desenvolvida pelo Facebook AI Research (FAIR). Oferece computação com tensores multidimensionais, diferenciação automática, modelos flexíveis, integração de GPU e uma ampla comunidade. Usado em visão computacional, processamento de linguagem natural, aprendizado de reforço e finanças quantitativas, PyTorch é uma ferramenta poderosa para construir e treinar modelos de última geração.

**Keras**: **Keras** é uma biblioteca de rede neural de alto nível. É fácil de usar, modular e extensível. Além disso, a biblioteca é construída sobre TensorFlow e suporta uma ampla gama de camadas, funções de ativação e funções de perda. 

**Características:**

* Interface intuitiva
* Construção modular
* Suporte a várias GPUs
* Ampla gama de camadas
* Funções de ativação e otimização

**Funções de Perda e Métricas:**

* Perda Quadrática Média (MSE): MSE = 1/n Σ(yᵢ - ŷᵢ)²
* Perda de Entropia Cruzada Binária (BCE): BCE = -Σ[yᵢ * log(ŷᵢ) + (1 - yᵢ) * log(1 - ŷᵢ)]
* Perda de Entropia Cruzada Categórica (CCE): CCE = -Σ[yᵢ * log(ŷᵢ)]

**Aplicações:**

Keras é usada em aplicações de aprendizado de máquina, como:

* Classificação de imagem
* Processamento de linguagem natural
* Detecção de objetos
* Série temporal

